%\section{Proof of Concept: X10 Refactoring Engine}
\section{Extract Concurrent}

As a first step toward our vision, we are developing a refactoring
called {\em extract
concurrent} for the X10 language within the X10DT, our Eclipse-based IDE. 
The transformation introduces concurrency within a loop by arranging
for some user-selected code in the loop body 
to run in parallel with other iterations of the 
loop.\footnote{The transformation can be thought of as
a fine-grained variant of loop distribution~\cite{loopdist} where only
a portion of the loop body is executed in parallel with other iterations.}

As an example, consider the X10 code in Figure~\ref{fig:CHM-X10}, an excerpt from
an X10 implementation of the {\tt ConcurrentHashMap} class from the Java 
standard utilities. In this snippet from the {\tt containsValue} method,
a snapshot of the modification status of each map segment is taken before
determining whether a particular segment contains the desired value. This
approach allows value lookup to occur without locking the entire map. Given
the PGAS model for data distribution, the individual elements of the array 
{\tt segments} could reside anywhere in the global address space, increasing
their access cost. Further, each call to {\tt modCount()} must block until the
call completes, per X10 semantics. Thus it is possible that asynchronously
executing the {\tt modCount} method for each array element will speed up the
overall execution of the loop.

\begin{figure}[tp]
  \begin{code}
int mcsum=0; \\
fo\=r ( point p : segments ) \{ \\
\>  mc[p] = segments[p].modCount(); \\
\>  mcsum += mc[p]; \\
\>  if\=(segments[p].containsValue(value)) \\
\>\>    return true; \\
\}
  \end{code}
\caption{\label{fig:CHM-X10} An excerpt from the X10 version of the Java 
library {\tt java.util.concurrent.ConcurrentHashMap}.  % It is 
% presented in a three address code form in order to simplify explanation.
}
\end{figure}

One way to introduce this concurrency, shown in Figure~\ref{fig:CHM-X10-future},
is to execute each of the {\tt modCount} invocations as {\tt future}s in a new loop
and only synchronize with those executions via a {\tt force}
operation when their results are actually needed.
This is in fact the transformation that was manually applied to
the code in our example after the initial translation
from Java to X10.  Our
{\em extract concurrent} refactoring automates this transformation and
ensures that the transformation preserves program behavior.  Our
refactoring also supports a generalization of this transformation that
allows a block of statements to be safely executed asynchronously,
but we do not discuss it here for brevity's sake.

% There are two ways in which this can be done.
%The first is to create a
%finished asynchronous {\tt foreach} loop that executes the method, but
%this causes a non-ideal situation where the rest of the loop must wait until
%every method invocation is cached before execution. 

% We chose to implement our refactoring scheme in the X10 language. X10 not only
% has a PGAS data consistency model, but also includes explicit higher-level
% concurrency constructs such as asynchronous blocks and future
% expressions. These concurrency constructs further simplify the analysis of X10
% code by making all asynchronous and atomic code apparent to both the analysis
% engine and the programmer.

%%\begin{figure}[tp]
%%  \begin{code}
%%    int mcsum=0; \\
%%    fo\=r (i=0; i<segments.length; i++)\{ \\
%%    \>  mcsum += mc[i] = segments[i].modCount(); \\
%%    \>  if\=(segments[i].containsValue(value)) \\
%%    \>\>    return true; \\
%%    \} \\
%%  \end{code}
%%\caption{\label{fig:CHM} A Java code excerpt from the library class {\tt
%%java.util.concurrent.ConcurrentHashMap} which illustrates a loop that
%%would not be parallelizable via traditional automatic loop
%%parallelization methods.}
%%\end{figure}

%%Because languages designed for the PGAS model do not focus on
%%providing support for the parallel execution of the same code over
% different sets of data, they can provide a more flexible alternative
% to whole loop parallelization: the loops can be executed sequentially
% and updates to distributed data structures can occur in parallel. 

%// segments.region $\rightarrow$ all future objects are {\em here}\\

\begin{figure}[tp]
  \begin{code}
int mcsum=0; \\
future<int>[.] f\_\=segments = \\
\>new future<int>[segments.region]; \\
fo\=r ( point p : segments ) \{ \\
\>  f\_\=segments[p] = \\
\>\>future(segments[p])\{segments[p].modCount()\}; \\
\} \\ \\
for ( point p : segments ) \{ \\
\>  mc[p] = f\_segments[p].force(); \\
\>  mcsum += mc[p]; \\
\>  if\=(segments[p].containsValue(value)) \\
\>\>    return true; \\
\} 
  \end{code}
\caption{\label{fig:CHM-X10-future} A transformation of the program excerpt 
from Figure~\ref{fig:CHM-X10} introducing additional concurrency via the 
X10 {\tt future} construct.}
\end{figure}

%\subsection{Extract Concurrent Refactoring}

%To further demonstrate our proof of concept, we are developing a refactoring
%which we call {\em Extract Concurrent}.


% We present in this paper the source code transformation {\em extract concurrent} for
% the X10 language which allows a programmer to transform loop code
% like that in Figure~\ref{fig:CHM-X10} to one that takes maximum advantage of
% asynchronous execution. 

Our refactoring involves two main components:
\begin{enumerate}
\item {\em Loop dependence analysis.} Since introducing parallelism in
the middle of a loop might affect the ability of other statements in a
loop to evaluate properly, it is important that loops do not depend
on the results of any asynchronously executed statements. We
have developed a set of analyses to determine whether {\em extract
concurrent} will adversely affect the execution of the code and
violate its perceived sequential consistency.

\item {\em Transformation pattern.} We have developed a general
pattern for the {\em extract concurrent} transformation on viable
sequential loops.
%finished asynchronous loops or future-caching loops.
The pattern splits the loop in two, as shown in Figure~\ref{fig:CHM-X10-future}: the
first loop introduces the desired statement- and/or expression-level
parallelism, while the second loop synchronizes with and utilizes the
results of the asynchronous execution.
% Because this split requires some code duplication, we
% present the results of a formal analysis on how the transformation
% affects the runtime of the loop and define the conditions under which
% the transformation provides the potential for better runtime. 
In practice, this
 transformation is more widely applicable to multiple statements or expressions.
We present here only the single statement or expression case.
\end{enumerate}

%%The primary goal of the PGAS model is to allow parallel asynchronous activity to
%%occur among multiple concurrent platforms, each having its own local memory and
%%potentially lacking a joint shared memory, without losing the ability to read
%%and write to global data. Global 
% shared data is actually owned by locally by processing units but is globally
% addressable. Other processors which would like
% to access or update global data then communicate directly with the owning
% location to perform any necessary actions. Thus, programmers take an active role
% in defining how data in arrays or data structures is distributed
% over the address space so as to maximize locality, thus taking maximum advantage
% of their concurrent environments. This removes the programmer's (or analysis')
% burden of determining
% how complicated structures should be partitioned among execution sites at
% points in the code where parallelism is desired (e.g., inside loops). 

The concurrency
constructs and the PGAS model used by X10 simplify the analysis required to determine
when program transformations are safe. For example, the static analysis required to
determine whether a statement may be executed asynchronously is
reduced to determining local and loop-carried dependencies that prevent a
statement from being asynchronously executed. With a traditional model, this
same transformation might also require an analysis to determine how much
dependent data would need to be copied for asynchronous execution and
where that execution should take place. The example highlights a perceived benefit of
refactoring X10: data locality and asynchronous execution are separable, or
{\em lateral}, concerns. Thus, a programmer may manipulate the amount of program
concurrency while keeping the data distribution fixed, or manipulate the
distribution while keeping perceived program concurrency relatively unchanged.

% \bug{This statement really applies
% to this particular transformation. I.e., the programmer does have to determine the partitioning at some point,
% but doesn't need to worry about it while applying this transformation. So
% perhaps we should say that we envision different kinds of {\em lateral} moves,
% e.g.: manipulate concurrency while keeping the distribution fixed, and
% manipulating the distribution while keeping concurrency relatively unchanged.}

We have built a prototype of the {\em extract concurrent} 
refactoring and the supporting analysis
in the X10DT, and we are in the process of refining the
implementation and experimenting with it on some X10 applications. 
It is our hope that 
the analysis itself will prove useful in implementing future refactorings,
and although the present implementation targets X10, we believe that the
transformation is readily applicable to other languages with a PGAS
programming model.
% \bug{We will insert more about the implementation when it's actually been done.}
